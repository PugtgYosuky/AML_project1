{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = os.listdir(dirs_path)\n",
    "\n",
    "# dirs = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time',\n",
       "       'param_criterion', 'param_splitter', 'params',\n",
       "       'split0_test_f1_weighted', 'split1_test_f1_weighted',\n",
       "       'split2_test_f1_weighted', 'split3_test_f1_weighted',\n",
       "       'split4_test_f1_weighted', 'mean_test_f1_weighted',\n",
       "       'std_test_f1_weighted', 'rank_test_f1_weighted',\n",
       "       'split0_test_balanced_accuracy', 'split1_test_balanced_accuracy',\n",
       "       'split2_test_balanced_accuracy', 'split3_test_balanced_accuracy',\n",
       "       'split4_test_balanced_accuracy', 'mean_test_balanced_accuracy',\n",
       "       'std_test_balanced_accuracy', 'rank_test_balanced_accuracy',\n",
       "       'split0_test_matthews_corrcoef', 'split1_test_matthews_corrcoef',\n",
       "       'split2_test_matthews_corrcoef', 'split3_test_matthews_corrcoef',\n",
       "       'split4_test_matthews_corrcoef', 'mean_test_matthews_corrcoef',\n",
       "       'std_test_matthews_corrcoef', 'rank_test_matthews_corrcoef',\n",
       "       'split0_test_recall_weighted', 'split1_test_recall_weighted',\n",
       "       'split2_test_recall_weighted', 'split3_test_recall_weighted',\n",
       "       'split4_test_recall_weighted', 'mean_test_recall_weighted',\n",
       "       'std_test_recall_weighted', 'rank_test_recall_weighted',\n",
       "       'split0_test_precision_weighted', 'split1_test_precision_weighted',\n",
       "       'split2_test_precision_weighted', 'split3_test_precision_weighted',\n",
       "       'split4_test_precision_weighted', 'mean_test_precision_weighted',\n",
       "       'std_test_precision_weighted', 'rank_test_precision_weighted'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"./logs/exp74/grid_search/DecisionTreeClassifier_grid_search_1680180611.9436672.json\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_config(path, columns):\n",
    "    path = os.path.join(path, 'grid_search')\n",
    "    metrics = ['f1_weighted','matthews_corrcoef', 'balanced_accuracy']\n",
    "    aux = pd.DataFrame()\n",
    "    for file in os.listdir(path):\n",
    "        df = pd.read_csv(os.path.join(path, file))\n",
    "        data = pd.DataFrame()\n",
    "        filename = file.split('_')[0]\n",
    "        data['config'] = df['params']\n",
    "        data['model'] = filename\n",
    "        for metric in metrics:\n",
    "            data[f'avg_{metric}'] = df[f'mean_test_{metric}']\n",
    "            data[f'std_{metric}'] = df[f'std_test_{metric}']\n",
    "        data = data[columns]\n",
    "        sorted_results = data.sort_values(by=['avg_matthews_corrcoef', 'avg_f1_weighted'])\n",
    "        sorted_results = sorted_results.iloc[:min(len(sorted_results), 5)]\n",
    "        aux = pd.concat([aux, sorted_results], ignore_index=True)\n",
    "    return aux\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_metric(path):\n",
    "    path = os.path.join(path, 'model_metrics.csv')\n",
    "    if not os.path.exists(path):\n",
    "        return pd.DataFrame()\n",
    "    data = pd.read_csv(path)\n",
    "    \n",
    "    mean = data.groupby(['model','config']).mean()\n",
    "    mean = mean.reset_index()\n",
    "\n",
    "    std =  data.groupby(['model','config']).std()\n",
    "    std = std.reset_index()\n",
    "    metrics = ['f1_weighted','matthews_corrcoef', 'balanced_accuracy']\n",
    "\n",
    "    new_data = pd.DataFrame()\n",
    "    new_data['model'] = mean['model']\n",
    "    new_data['config'] = mean['config']\n",
    "\n",
    "    for i in metrics:\n",
    "        new_data[\"avg_\"+i] = mean[i]\n",
    "        new_data[\"std_\"+i] = std[i]\n",
    "\n",
    "    return new_data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model', 'config', 'avg_f1_weighted', 'std_f1_weighted', 'avg_matthews_corrcoef', 'std_matthews_corrcoef', 'avg_balanced_accuracy', 'std_balanced_accuracy']\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "metrics = ['f1_weighted','matthews_corrcoef', 'balanced_accuracy']\n",
    "columns = ['model', 'config']\n",
    "\n",
    "for metric in metrics:\n",
    "        columns.append(f'avg_{metric}')\n",
    "        columns.append(f'std_{metric}')\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp00\n",
      "exp01\n",
      "exp02\n",
      "exp03\n",
      "exp04\n",
      "exp05\n",
      "exp06\n",
      "exp07\n",
      "exp08\n",
      "exp09\n",
      "exp10\n",
      "exp41\n",
      "exp51\n",
      "exp62\n",
      "exp63\n",
      "exp64\n",
      "exp65\n",
      "exp66\n",
      "exp67\n",
      "exp68\n",
      "exp69\n",
      "exp70\n",
      "exp71\n",
      "exp72\n",
      "exp73\n",
      "exp74\n",
      "exp75\n",
      "exp76\n",
      "exp77\n",
      "exp78\n",
      "exp79\n",
      "exp80\n",
      "exp83\n",
      "exp84\n",
      "exp85\n",
      "exp86\n"
     ]
    }
   ],
   "source": [
    "for d in dirs:\n",
    "    path = os.path.join(dirs_path, d)\n",
    "    print(d)\n",
    "    if d == 'exp00':\n",
    "        continue\n",
    "    with open(os.path.join(path, 'config.json'), 'r') as file:\n",
    "        config = json.load(file)\n",
    "        if config.get('grid_search', False):\n",
    "            df = get_data_from_config(path, columns)\n",
    "            data = pd.concat([data, df], ignore_index=True)\n",
    "        else:\n",
    "            df = get_data_from_metric(path)\n",
    "            data = pd.concat([data, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('results_mean_std.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
